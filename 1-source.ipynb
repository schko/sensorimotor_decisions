{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f96671aa",
   "metadata": {},
   "source": [
    "# Analyze sessions in batch from Phase 1 of AdaDrive (work in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261d0746",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "path_to_base_package = '../..'\n",
    "import sys\n",
    "# setting path\n",
    "sys.path.append(f\"{path_to_base_package}\")\n",
    "import mne\n",
    "mne.viz.set_3d_backend('pyvistaqt')\n",
    "mne.viz.set_3d_options(antialias=False) \n",
    "\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from mna.utils.data_access import *\n",
    "from mna.utils.analysis import *\n",
    "from mne.datasets import fetch_fsaverage\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "# Download fsaverage files\n",
    "fs_dir = fetch_fsaverage(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c13166b",
   "metadata": {},
   "source": [
    "# Aux functions, read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4608af72-c0ae-443f-9e53-b6cea0773f01",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "output_dir = f\"{path_to_base_package}/output/batch_analysis_motor/\"\n",
    "output_dir_non_baseline_non_average = f\"{path_to_base_package}/output/batch_analysis_motor/saved_files_non_baseline_non_average/\" # saved files directory (trial or motor)\n",
    "remove_sessions = [(15,1),(22,1)]\n",
    "rel_regions, all_region = get_relevant_channels()\n",
    "rel_labels, rel_mappings = get_relevant_labels_mappings(path_to_base_package)\n",
    "\n",
    "pupil_df = pd.read_csv(f\"{path_to_base_package}/output/pupil_exposure/participant_level_exposure_fits.csv\")\n",
    "trial_dfs = pd.read_csv(f\"{output_dir}all_results.csv\")\n",
    "motor_dfs = read_motor_csvs(output_dir)\n",
    "motor_dfs['post_steer_event_raw'] = motor_dfs['post_steer_event_raw'].apply(str_list_to_list)\n",
    "motor_epochs = get_motor_epochs(output_dir_non_baseline_non_average)\n",
    "#low_motor_sensor = motor_epochs[\"Steer_Wheel_Degree_Categorical == 'Low'\"]\n",
    "#high_motor_sensor = motor_epochs[\"Steer_Wheel_Degree_Categorical == 'High'\"]\n",
    "#low_pupil = motor_epochs[\"pupil_bin == '{}'\".format('low')]\n",
    "#high_pupil =motor_epochs[\"pupil_bin == '{}'\".format('high')]\n",
    "exposure_epochs = get_exposure_epochs(f\"{path_to_base_package}/output/exposure/exposure_epochs.pickle\")\n",
    "#low_motor_sensor.apply_proj()\n",
    "#high_motor_sensor.apply_proj()\n",
    "\n",
    "p_val_criteria = 0.05\n",
    "preturn = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36208051",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Clean up dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a9aafc-5532-4331-94fc-d568a049d0b8",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# seaborn\n",
    "import math \n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_palette(\"tab10\")\n",
    "from mna.utils.batch_feature_extraction import clean_up_adadrive_trials\n",
    "\n",
    "motor_outlier_cols = ['abs_sum_delta_steer_input']\n",
    "cols_to_outlier_detect = ['bpm', 'sdnn', 'rmssd', 'pnn50']\n",
    "experimental_cols = ['spoken_difficulty', 'trial_duration', 'density', 'trial_damage']\n",
    "eye_cols = ['Left Pupil Diameter', \"NSLR_count_Fixation\", \"NSLR_count_Saccade\",\n",
    "            'NSLR_mean_duration_Fixation', 'NSLR_mean_duration_Saccade',\n",
    "            'NSLR_first_onset_Fixation', 'NSLR_first_onset_Saccade']\n",
    "ecg_cols = ['bpm', 'sdnn', 'rmssd', 'pnn50']  # rmssd = parasympathetic\n",
    "motor_cols = ['abs_sum_delta_steer_input', 'abs_sum_delta_brake_input', 'abs_sum_delta_throttle_input']\n",
    "def remove_motor_overlaps(test_df):\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "    trial_list = list(zip(test_df.trial_start_time, test_df.trial_end_time, test_df.index))\n",
    "    overlaps = []\n",
    "    for i in range(1,len(trial_list)):\n",
    "        base_data = trial_list[i]\n",
    "        check_data = trial_list[i-1]\n",
    "        if base_data[1] > check_data[0] and base_data[1] <= check_data[1]:\n",
    "            overlaps.append((base_data, check_data, base_data[1]-check_data[0], base_data[2], check_data[2]))\n",
    "            assert base_data[1]-check_data[0] != 1, 'Major issue, repeating trials found. Double check'\n",
    "    drop_rows = []\n",
    "    for overlap in overlaps:\n",
    "        row_1 = test_df.iloc[overlap[-1]]\n",
    "        row_2 = test_df.iloc[overlap[-2]]\n",
    "        if np.argmax([row_1.Abs_Steer_Wheel_Degree, row_2.Abs_Steer_Wheel_Degree]) == 0: # if row 1 has larger steer motion, drop the other\n",
    "            drop_rows.append(overlap[-2])\n",
    "        else:\n",
    "            drop_rows.append(overlap[-1])\n",
    "    test_df = test_df.drop(drop_rows,axis=0)\n",
    "    return test_df\n",
    "\n",
    "def clean_up_trials(input_df):\n",
    "    all_dfs_final = clean_up_adadrive_trials(input_df.copy())\n",
    "    # damage change\n",
    "    all_dfs_final = all_dfs_final.sort_values(by=['ppid', 'session', 'block', 'trial'])\n",
    "    # nan, outliers\n",
    "\n",
    "    all_dfs_final['NSLR_first_onset_Fixation'] = all_dfs_final['NSLR_first_onset_Fixation'] - all_dfs_final[\n",
    "        'trial_start_time']\n",
    "    all_dfs_final['NSLR_first_onset_Saccade'] = all_dfs_final['NSLR_first_onset_Saccade'] - all_dfs_final[\n",
    "        'trial_start_time']\n",
    "\n",
    "    all_dfs_final[\n",
    "        'throttle_over_brake'] = all_dfs_final.abs_sum_delta_throttle_input / all_dfs_final.abs_sum_delta_brake_input\n",
    "    return all_dfs_final\n",
    "\n",
    "\n",
    "trial_dfs = clean_up_trials(trial_dfs)\n",
    "trial_dfs = trial_dfs.loc[~trial_dfs.ppid_session.isin([f\"{es[0]}_{es[1]}\" for es in remove_sessions])]\n",
    "motor_dfs = clean_up_trials(motor_dfs)\n",
    "print(f\"removing ovlerlapping motor trials, starting epoch count {len(motor_dfs)}\")\n",
    "motor_dfs = remove_motor_overlaps(motor_dfs)\n",
    "print(f\"post removal epoch count {len(motor_dfs)}\")\n",
    "# luminance effect removal from pupil diameter\n",
    "trial_dfs['Raw Left Pupil Diameter'] = trial_dfs['Left Pupil Diameter']\n",
    "motor_dfs['Raw Left Pupil Diameter'] = motor_dfs['Left Pupil Diameter']\n",
    "trial_dfs = trial_dfs.reset_index(drop=True)\n",
    "adjustments=[]\n",
    "for index, row in trial_dfs.iloc[1:].iterrows():\n",
    "    last_ppid = trial_dfs.iloc[index - 1].ppid\n",
    "    last_session = trial_dfs.iloc[index - 1].session\n",
    "    last_trial = trial_dfs.iloc[index - 1].trial\n",
    "    last_opacity = trial_dfs.iloc[index - 1].density\n",
    "    if ((row.ppid == last_ppid) & (row.session == last_session) & (row.trial == last_trial + 1)):  # if continuous\n",
    "        # if there is a significant effect of opacity on pupil\n",
    "        if pupil_df.loc[pupil_df['sub'] == last_ppid, 'p_opacities'].values < p_val_criteria:\n",
    "            this_opacity = row.density\n",
    "            this_pupil_diameter = row['Left Pupil Diameter']\n",
    "            weight = pupil_df.loc[pupil_df['sub'] == last_ppid, 'w_opacities']\n",
    "            adjustment = ((this_opacity - last_opacity) * weight).values[0]\n",
    "            trial_dfs.iloc[index, trial_dfs.columns.get_loc('Left Pupil Diameter')] += adjustment\n",
    "            # this needs to be converted to array b/c of pandas issues\n",
    "            old_pupil_value = np.array(motor_dfs.loc[(motor_dfs.ppid == last_ppid) & (motor_dfs.session == last_session) & (\n",
    "                        motor_dfs.trial == last_trial + 1), 'Left Pupil Diameter']) \n",
    "            motor_dfs.loc[(motor_dfs.ppid == last_ppid) & (motor_dfs.session == last_session) & (\n",
    "                        motor_dfs.trial == last_trial + 1), 'Left Pupil Diameter'] = (old_pupil_value-adjustment).T  # update motor df too\n",
    "            # do also for motor_epochs\n",
    "            old_pupil_value = motor_epochs.metadata.loc[(motor_epochs.metadata.ppid == last_ppid) &\n",
    "                                      (motor_epochs.metadata.session == last_session) &\n",
    "                                      (motor_epochs.metadata.trial == last_trial + 1), 'Left Pupil Diameter']\n",
    "            motor_epochs.metadata.loc[(motor_epochs.metadata.ppid == last_ppid) &\n",
    "                                      (motor_epochs.metadata.session == last_session) &\n",
    "                                      (motor_epochs.metadata.trial == last_trial + 1), 'Left Pupil Diameter'] = (old_pupil_value-adjustment).T\n",
    "# pupil bins\n",
    "motor_dfs['pupil_bin'] = motor_dfs.groupby(['ppid'])['Left Pupil Diameter'].transform(\n",
    "    lambda x: pd.qcut(x, 2, labels=['low', 'high']))\n",
    "trial_dfs['pupil_bin'] = trial_dfs.groupby(['ppid'])['Left Pupil Diameter'].transform(\n",
    "    lambda x: pd.qcut(x, 2, labels=['low', 'high']))\n",
    "motor_epochs.metadata['pupil_bin'] = motor_epochs.metadata.groupby(['ppid'])['Left Pupil Diameter'].transform(\n",
    "    lambda x: pd.qcut(x, 2, labels=['low', 'high']))\n",
    "motor_dfs['pupil_bin_encoded'] = motor_dfs.groupby(['ppid'])['Left Pupil Diameter'].transform(\n",
    "    lambda x: pd.qcut(x, 2, labels=[0, 1]))\n",
    "trial_dfs['pupil_bin_encoded'] = trial_dfs.groupby(['ppid'])['Left Pupil Diameter'].transform(\n",
    "    lambda x: pd.qcut(x, 2, labels=[0, 1]))\n",
    "motor_epochs.metadata['pupil_bin_encoded'] = motor_epochs.metadata.groupby(['ppid'])['Left Pupil Diameter'].transform(\n",
    "    lambda x: pd.qcut(x, 2, labels=[0, 1]))\n",
    "\n",
    "# participant-level binning of motor data, replaces the session-level info already there\n",
    "motor_dfs = get_motor_intensity_info(motor_dfs)\n",
    "motor_epochs.metadata = get_motor_intensity_info(motor_epochs.metadata)\n",
    "\n",
    "\n",
    "# ensure that epochs that removed from motor epochs are also removed from EEG analysis (we don't do the reverse since we have non-eeg, usable data)\n",
    "def get_merged_motor_epochs(input_mepochs, input_motor_dfs):\n",
    "    df_all = input_mepochs.metadata[['ppid', 'session', 'block', 'trial']].merge(input_motor_dfs[['ppid', 'session', 'block', 'trial']].drop_duplicates(), on=['ppid', 'session', 'block', 'trial'], \n",
    "                    how='inner', indicator=True)\n",
    "    i1 = input_mepochs.metadata.set_index(['ppid', 'session', 'block', 'trial']).index\n",
    "    i2 = df_all.set_index(['ppid', 'session', 'block', 'trial']).index\n",
    "    input_mepochs = input_mepochs[i1.isin(i2)]\n",
    "    # motor_epochs.set_eeg_reference('average') # custom eeg reference is not allowed for MNE source modeling\n",
    "    input_mepochs.apply_baseline((-((preturn+250) / 1000), -((preturn) / 1000)))\n",
    "    return input_mepochs\n",
    "motor_epochs = get_merged_motor_epochs(motor_epochs, motor_dfs)\n",
    "print(pupil_df[['sub','w_opacities','const','p_opacities']].to_latex(index=False,float_format=\"{:0.2f}\".format))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94072f52",
   "metadata": {},
   "source": [
    "# eLORETA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d6d686",
   "metadata": {},
   "source": [
    "## Load forward model, create inverse operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b4e603",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "subjects_dir = os.path.dirname(fs_dir)\n",
    "\n",
    "# The files live in:\n",
    "subject = 'fsaverage'\n",
    "trans = 'fsaverage'  # MNE has a built-in fsaverage transformation\n",
    "'''\n",
    "select the boundary element model, note that the source data has been downsampled by a factor of 5 \n",
    "(i.e. ico == 5, https://mne.tools/stable/generated/mne.setup_source_space.html#mne.setup_source_space)\n",
    "and the BEM has been downsampled by a factor of 5 (i.e. ico == 4, see here: https://mne.tools/stable/generated/mne.make_bem_model.html)\n",
    "implications here: https://brainder.org/2016/05/31/downsampling-decimating-a-brain-surface/\n",
    "'''\n",
    "src_fname = os.path.join(fs_dir, 'bem', 'fsaverage-ico-5-src.fif')\n",
    "bem = os.path.join(fs_dir, 'bem', 'fsaverage-5120-5120-5120-bem-sol.fif')\n",
    "\n",
    "eeg_montage='biosemi64'\n",
    "info = motor_epochs.info\n",
    "\n",
    "# Read and set the EEG electrode locations, which are already in fsaverage's\n",
    "# space (MNI space) for standard_1020:\n",
    "montage = mne.channels.make_standard_montage(eeg_montage)\n",
    "\n",
    "# Check that the locations of EEG electrodes is correct with respect to MRI\n",
    "# mne.viz.plot_alignment(\n",
    "#    info, src=src_fname, eeg=['original', 'projected'], trans=trans,\n",
    "#    show_axes=False, mri_fiducials=True, dig='fiducials')\n",
    "fwd = mne.make_forward_solution(info, trans=trans, src=src_fname,\n",
    "                                bem=bem, eeg=True, n_jobs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bbd6eb",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "method = \"eLORETA\"\n",
    "snr = 3.\n",
    "lambda2 = 1. / snr ** 2\n",
    "cov = mne.compute_covariance(exposure_epochs, method='auto') # note this is not average referenced\n",
    "cov.plot(exposure_epochs.info)\n",
    "inverse_operator = mne.minimum_norm.make_inverse_operator(\n",
    "    info, fwd, cov)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c64ad57b",
   "metadata": {},
   "source": [
    "# Get time courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e539a2-0700-46be-b589-37843271337b",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_time_courses(input_epochs, input_labels):\n",
    "    # Average the source estimates within each label of the cortical parcellation\n",
    "    # and each sub-structure contained in the source space.\n",
    "    # When mode = 'mean_flip', this option is used only for the cortical labels.\n",
    "    src = inverse_operator['src']\n",
    "    rel_stcs = mne.minimum_norm.apply_inverse_epochs(input_epochs, inverse_operator,\n",
    "                                    lambda2=1.0 / snr ** 2, verbose=False,\n",
    "                                    method=\"eLORETA\", pick_ori=\"normal\")\n",
    "    label_ts = mne.extract_label_time_course(\n",
    "        rel_stcs, input_labels, src, mode='mean_flip', allow_empty=True,\n",
    "        return_generator=True, verbose=False)\n",
    "    return label_ts\n",
    "\n",
    "def get_all_tcs(output_dir, overwrite=False):\n",
    "    if not overwrite and os.path.isfile(f\"{output_dir}source_time_courses.pickle\"):\n",
    "        all_cond_tcs_df = pickle.load(open(f\"{output_dir}source_time_courses.pickle\", 'rb'))\n",
    "        return all_cond_tcs_df\n",
    "    else:\n",
    "        all_cond_tcs = []\n",
    "        pps = motor_epochs.metadata['ppid'].unique()\n",
    "        for this_pid in pps:\n",
    "            print('this_pid',this_pid)\n",
    "            this_pid_df = motor_epochs.metadata[motor_epochs.metadata['ppid']==this_pid]\n",
    "            nested_df = this_pid_df[this_pid_df['Steer_Wheel_Degree_Categorical']=='Low']\n",
    "            st = set(nested_df.trial_start_time)\n",
    "            relevant_indices = [i for i, e in enumerate(motor_epochs.metadata.trial_start_time) if e in st]\n",
    "            low_pp_epochs = motor_epochs[relevant_indices] # note that motor_epochs index is not the same as the motor_epochs.metadata df index so we need to do this\n",
    "\n",
    "            nested_df = this_pid_df[this_pid_df['Steer_Wheel_Degree_Categorical']=='High']\n",
    "            st = set(nested_df.trial_start_time)\n",
    "            relevant_indices = [i for i, e in enumerate(motor_epochs.metadata.trial_start_time) if e in st]\n",
    "            high_pp_epochs = motor_epochs[relevant_indices] # note that motor_epochs index is not the same as the motor_epochs.metadata df index so we need to do this\n",
    "\n",
    "            ret_tcs = get_time_courses(low_pp_epochs, rel_labels)\n",
    "            global_trials = list(low_pp_epochs.metadata.trial)\n",
    "            global_trial_starts = list(low_pp_epochs.metadata.trial_start_time)\n",
    "            global_sessions = list(low_pp_epochs.metadata.session)\n",
    "            all_low_tcs = []\n",
    "            for indx, t in enumerate(ret_tcs):\n",
    "                trial_df = pd.DataFrame(t)\n",
    "                trial_df = trial_df.T\n",
    "                trial_df.columns = [l.name for l in rel_labels]\n",
    "                trial_df['motor_event_trial'] = indx\n",
    "                trial_df['trial'] = global_trials[indx]\n",
    "                trial_df['trial_start_time'] = global_trial_starts[indx]\n",
    "                trial_df['session'] = global_sessions[indx]\n",
    "                trial_df['sample'] = trial_df.index\n",
    "                trial_df['pid'] = this_pid\n",
    "                all_low_tcs.append(trial_df)\n",
    "            all_low_tcs = pd.concat(all_low_tcs)\n",
    "            all_low_tcs['cond'] = 'low'\n",
    "\n",
    "            ret_tcs = get_time_courses(high_pp_epochs,rel_labels)\n",
    "            global_trials = list(high_pp_epochs.metadata.trial)\n",
    "            global_trial_starts = list(high_pp_epochs.metadata.trial_start_time)\n",
    "            global_sessions = list(high_pp_epochs.metadata.session)\n",
    "            all_high_tcs = []\n",
    "            for indx, t in enumerate(ret_tcs):\n",
    "                trial_df = pd.DataFrame(t)\n",
    "                trial_df = trial_df.T\n",
    "                trial_df.columns = [l.name for l in rel_labels]\n",
    "                trial_df['motor_event_trial'] = indx\n",
    "                trial_df['trial'] = global_trials[indx]\n",
    "                trial_df['trial_start_time'] = global_trial_starts[indx]\n",
    "                trial_df['session'] = global_sessions[indx]\n",
    "                trial_df['sample'] = trial_df.index\n",
    "                trial_df['pid'] = this_pid\n",
    "                all_high_tcs.append(trial_df)\n",
    "            all_high_tcs = pd.concat(all_high_tcs)\n",
    "            all_high_tcs['cond'] = 'high'\n",
    "            all_cond_tcs.append(pd.concat([all_low_tcs,all_high_tcs]))\n",
    "            \n",
    "        all_cond_tcs_df = pd.concat(all_cond_tcs)\n",
    "        all_cond_tcs_df = pd.melt(all_cond_tcs_df, id_vars=['motor_event_trial','trial','trial_start_time','session','sample','pid','cond'],value_name='activation',var_name='source_region')\n",
    "        with open(f\"{output_dir}source_time_courses.pickle\", 'wb') as handle_ica:\n",
    "            pickle.dump(all_cond_tcs_df, handle_ica, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return all_cond_tcs_df\n",
    "\n",
    "\n",
    "all_tcs = get_all_tcs(output_dir_non_baseline_non_average,overwrite=False)\n",
    "all_tcs['hemi'] = all_tcs.source_region.apply(lambda x: x.split('-')[1])\n",
    "all_tcs['source_region'] = all_tcs.source_region.apply(lambda x: rel_mappings[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1891258b",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_epoched_tcs(all_tcs_tcs,relvant_labels,relvant_mappings):\n",
    "    '''\n",
    "    Returns long form of all_tcs data, can be filtered input\n",
    "    '''\n",
    "    all_dfs = []\n",
    "    all_tcs_src = defaultdict(list)\n",
    "    for name,group in all_tcs_tcs.groupby(['pid','motor_event_trial', 'cond', 'trial','source_region','hemi']):  # loop through each epoch\n",
    "        group['baseline_corr_activation'] = group.activation - np.mean(group.activation.values[0:int(.25*128)]) # save baseline-adjusted data\n",
    "        all_dfs.append(group)\n",
    "        all_tcs_src[(group.source_region.iloc[0],group.hemi.iloc[0])].append(group.activation.values)\n",
    "    all_tcs_tcs = pd.concat(all_dfs)\n",
    "    all_tcs_tcs['time'] = (all_tcs_tcs['sample']-(1.25*128))/128 # sample to time\n",
    "\n",
    "    ordered_input_tcs = []\n",
    "    for l in relvant_labels:\n",
    "        ordered_input_tcs.append(all_tcs_src[(relvant_mappings[l.name],l.name.split('-')[1])])\n",
    "    ordered_input_tcs = np.swapaxes(np.array(ordered_input_tcs),0,1)\n",
    "    return ordered_input_tcs, all_tcs_tcs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d141c84d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m95"
  },
  "kernelspec": {
   "display_name": "mna",
   "language": "python",
   "name": "mna"
  },
  "vscode": {
   "interpreter": {
    "hash": "967869b3d3e599d39c4e482f6852385da2dcc34e629cb6c89bbd952eba61abed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
